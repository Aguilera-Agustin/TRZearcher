{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Estructura de Datos</center>\n",
    "## <center>Universidad Nacional de Tres de Febrero</center>\n",
    "## <center>TRZearcher</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRABAJO PRACTICO - TRANZAS DEL CODIGO\n",
    "    Aguilera, Agustin\n",
    "    Ambrosetti, Ramiro\n",
    "    Noguera, Sol\n",
    "    Tabarovsky, Javier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRZearcher\n",
    "    Es un programa de busqueda y comparacion de precios de productos tecnologicos, para encontrar el mejor precio en el menor tiempo posible\n",
    "    Por el momento trabajamos con: \n",
    "        - Compragamer\n",
    "        - Full H4rd\n",
    "        - Gezatek\n",
    "        - Overdrive\n",
    "        - Venex\n",
    "    Estas paginas fueron seleccionadas debido a que son las mas economicas en su rubro a la fecha de publicacion de este informe, son paginas en continuo crecimiento, actualizadas diariamente, y con la mayor variedad de productos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UML - Diagrama de clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"UML.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TECNICAS DE PROGRAMACION Y DISEÑO\n",
    "    Las tecnicas de programacion utilizadas en este proyecto se basan en la programacion orientada a objetos, y la utilizacion de la libreria Scrapy.\n",
    "    El diseño del programa esta dividido en 2 procesos, el Front y Back, esto se realizo de esta manera para solventar un problema.\n",
    "    El problema se centraba en que al ejecutarse el programa, primero las spiders leian la informacion de configuracion y luego se ejecutaba la interfaz, esto ocasionaba que las \n",
    "    url de inicio se encontraban incompletas\n",
    "    El diseño se basa en la modularizacion de las funcionalidades, a continuacion se puede ver un diagrama donde explicamos como se desenvuelve el programa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Ilustracion.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El programa comienza su ejecucion desde la clase Main, la cual llama a Interface y la ejecuta:\n",
    "\n",
    "    1 - Una vez que el usuario eligio su configuracion de busqueda, y presiona el boton de buscar, estas configuraciones son guardadas en un temporal denominado temp_connector, y otro temporal con las url a las paginas completas denominado pages_complete.txt.\n",
    "    2 - Este temp_conector es leido por la clase Proyect Connector.                           \n",
    "    3 - La clase Proyect Connector, instancia a la clase Spider Connector la cual se encarga de :\n",
    "        3.1 - Instanciar a la clase Orchestrator, la cual ejecuta las spiders seleccionadas, y estas mismas exportan los datos en temporales.csv.\n",
    "        3.2 - Instanciar a la clase Sorter, la cual lee los temporales.csv, los ordena y filtra segun el tipo de busqueda, para luego instanciar a la clase Exporter con los resultados.\n",
    "    4 - La clase Exporter se encarga de escribir los resultados en dos formatos, csv, json y html.\n",
    "    Esta seria una explicacion a grandes rasgos del funcionamiento del programa       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTERFACE (FRONT)\n",
    "    La interfaz esta creada como un proceso separado del programa, con una unica clase la cual crea una interfaz grafica con el uso de la libreria tkinter.\n",
    "    En la cual a la hora de presionar el boton de buscar se realizan las siguientes acciones:\n",
    "    \n",
    "        1. Verifica que la entrada del producto no puede estar vacia, y al menos se debe seleccionar una pagina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __code_button(self):\n",
    "            if (str(self.product.get()).strip() == \"\") and (self.compra_gamer.get() + self.full_hard.get() + self.gezatek.get() + self.venex.get() + self.overdrive.get() <= 0):\n",
    "                messagebox.showerror(message=\"Recordar que:\\n - La entrada del producto no puede estar vacia\\n - Al menos hay que seleccionar una pagina\", title=\"Problema con TRZearcher\")\n",
    "            else:\n",
    "                self.__write_data_connector()\n",
    "                self.__write_pages()\n",
    "                subprocess.call(\"python ./Back/Connector/proyect_connector.py\", shell=True)\n",
    "                self.window.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        2. Escribe los datos de la interfaz en un temporal, llamado temp_connector.txt, el cual luego es leido por el otro proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __write_data_connector(self):\n",
    "        with open(\"./Back/Connector/temp_connector.txt\", 'w') as pages_file:\n",
    "            pages_file.write(str(self.product.get()) + \"\\n\")\n",
    "            pages_file.write(str(self.compra_gamer.get()) + \"\\n\")\n",
    "            pages_file.write(str(self.full_hard.get()) + \"\\n\")\n",
    "            pages_file.write(str(self.gezatek.get()) + \"\\n\")\n",
    "            pages_file.write(str(self.venex.get()) + \"\\n\")\n",
    "            pages_file.write(str(self.overdrive.get()) + \"\\n\")\n",
    "            pages_file.write(str(self.option.get()) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        3. Completa las paginas a buscar, leyendo de pages_initials.txt, agregando el producto al final y lo exporta a un temporal llamado page_complete.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __write_pages(self):\n",
    "        pages_complete = []\n",
    "        number_of_pages = 0\n",
    "        fileInitials = os.path.abspath(\"./Back/Searcher/Data/pages_initials.txt\")\n",
    "        fileComplete = os.path.abspath(\"./Back/Searcher/Data/pages_complete.txt\")\n",
    "        with open(fileInitials, \"r\") as pages_file:\n",
    "            for page in pages_file:\n",
    "                number_of_pages += 1\n",
    "                page = page.rstrip('\\n') + str(self.product.get()) + \"\\n\"\n",
    "                pages_complete.append(page)\n",
    "        with open(fileComplete, \"w\") as pages_file:\n",
    "            for x in range(0, number_of_pages):\n",
    "                pages_file.write(pages_complete[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Una vez finalizado, se ejecuta el siguiente proceso, y se cierra la ventana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONNECTOR (BACK)\n",
    "    El modulo connector esta creado para la conexion del programa, la cual esta compuesta de dos clases proyect_connector y spyder_connector.\n",
    "        1. proyect_connector: Se basa en la lectura de los datos obtenidos de interfaz (a travez del temporal temp_connector.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    __data = []\n",
    "    with open(\"./Back/Connector/temp_connector.txt\", \"r\") as data_connector:\n",
    "        for data in data_connector:\n",
    "            __data.append(str(data).strip())\n",
    "\n",
    "    Connector(__data[0], __data[1], __data[2], __data[3], __data[4], __data[5], __data[6]).execute_connector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        2. spyder_connector: Es la clase mas importante del programa, la cual es la encargada en primer instancia de verificar que paginas se quieren buscar, para proceder a ejecutar las Spiders, a su vez elimina los archivos temporales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class Connector:\n",
    "\n",
    "        def __init__(self, product, compragamer, fullhard, gezatek, venex, overdrive, type_search):\n",
    "            self.product = product\n",
    "            self.pages = self.__spiders_validation(compragamer, fullhard, gezatek, venex, overdrive)\n",
    "            self.type_search = type_search\n",
    "\n",
    "        def __spiders_validation(self, compragamer, fullhard, gezatek, venex, overdrive):\n",
    "            pages = []\n",
    "            if int(compragamer) == 1:\n",
    "                pages.append(\"compragamer\")\n",
    "            if int(fullhard) == 1:\n",
    "                pages.append(\"fullh4rd\")\n",
    "            if int(gezatek) == 1:\n",
    "                pages.append(\"gezatek\")\n",
    "            if int(overdrive) == 1:\n",
    "                pages.append(\"overdrive\")\n",
    "            if int(venex) == 1:\n",
    "                pages.append(\"venex\")\n",
    "            return pages\n",
    "\n",
    "        def execute_connector(self):\n",
    "            Orchestrator().execute_spiders(self.pages)\n",
    "            Sorter(self.pages, self.product, self.type_search).execute_sorter()\n",
    "            self.__clean()\n",
    "            sys.exit()\n",
    "\n",
    "        def __clean(self):\n",
    "            os.remove(\"./Back/Connector/temp_connector.txt\")\n",
    "            os.remove(\"./Back/Searcher/Data/pages_complete.txt\")\n",
    "            for page in self.pages:\n",
    "                os.remove(page + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROCESSOR (BACK)\n",
    "    Completar por Sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Codigo importante a explicar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEARCHER (BACK)\n",
    "    El modulo searcher es el encargado de recuperar los datos de la web, se divide a su vez en:\n",
    "        1. Config: Configuracion de scrapy, con sus items, etc.\n",
    "        2. Data: Archivos temporales y estaticos, los cuales son utilizados por las arañas para su funcionamiento.\n",
    "        3. Spiders: Son las encargadas de recuperar la informacion de determinadas paginas, y exportarlas a csv temporales. Este modulo incluye su orchestrator, el cual las ejecuta.\n",
    "        Un ejemplo podria ser la CompragamerSpider, las demas son similares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    La primer parte de cada spider, se basa en su configuracion:\n",
    "        1. name: Nombre de la araña.\n",
    "        2. custom_settings: Forma de exportacion de los datos, en este caso csv.\n",
    "        3. allowed_domains: Referencia al dominio en el cual se va a trabajar, del cual no se puede salir.\n",
    "        4. item_count: Es una variable que utilizaremos mas adelante para limitar la salida de datos, y que el programa no tarde demasiado tiempo.\n",
    "        5. start_urls: Links de las paginas desde donde se va a lanzar a recolectar datos, en este caso se toman desde un archivo temporal.\n",
    "        6. rules: Reglas que la spider debe tener en cuenta en este caso, es que cada vez que encuentre un elemento de la lista (xpaths), entre a el dominio ejecutando parse_items,\n",
    "            esto es muy util ya que al entrar a cada publicacion se puede extraer informacion que no se encuentra en la lista de la url incial. Otra regla podria ser que cada vez que encuentre\n",
    "            un xpaths de un boton (por ejemplo siguiente pagina), siga esa url, para continuar con la busqueda.\n",
    "        Por ultimo elimina el archivo temporal anterior, en el caso de que todavia siga existiendo (esto se hace si por alguna razon el programa se interrumpe en su proceso)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class CompragamerSpider(CrawlSpider):\n",
    "        \"\"\"\n",
    "            Spider que recolecta datos de la pagina www.compragamer.com, con un limite de 15\n",
    "        \"\"\"\n",
    "\n",
    "        name = 'compragamer'\n",
    "\n",
    "        custom_settings = {'FEEDS': {'compragamer.csv': {'format': 'csv'}}}\n",
    "\n",
    "        allowed_domains = ['compragamer.com']\n",
    "\n",
    "        item_count = 0\n",
    "\n",
    "        url = \"\"\n",
    "        filePath = os.path.abspath(\"../TRZearcher/Back/Searcher/Data/pages_complete.txt\")\n",
    "        with open(filePath, \"r\") as pages:\n",
    "            for page in pages:\n",
    "                if page.find(\"compragamer\") > 0:\n",
    "                    url = page\n",
    "\n",
    "        start_urls = [url]\n",
    "\n",
    "        rules = {\n",
    "            Rule(LinkExtractor(allow=(), restrict_xpaths=('//*[@class=\"products__item\"]')),\n",
    "                 callback='parse_item', follow=False)\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            os.remove('compragamer.csv')\n",
    "        except OSError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    La segunda parte de cada spider, se basa en la recuperacion de datos:\n",
    "        - La llamada al TrzpidersItem se da para guardar los elementos que vamos a recuperar.\n",
    "        - Cada elemento se recupera en mediante css (menos link y time), y dependiendo como se encuentre se arregla para su posterior salida en csv.\n",
    "        - Los elementos a extraer por pagina se limitan a un maximo de 15 elementos, para que el programa no tarde demasiado tiempo y recupere muchos elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def parse_item(self, response):\n",
    "                \"\"\"\n",
    "                    Recolecta la informacion de cada item\n",
    "                \"\"\"\n",
    "\n",
    "                item = TrzpidersItem()\n",
    "\n",
    "                item['title'] = str(response.css(\n",
    "                    '.filaNombre div::text').extract_first()).strip()\n",
    "\n",
    "                item['price'] = str(response.css(\n",
    "                    '.col-xs-5::text').extract_first()).split(\"$\")[1].strip()\n",
    "\n",
    "                item['category'] = str(response.css(\n",
    "                    '.detalleNombre .product-card__name::text').extract_first()).capitalize().strip()\n",
    "\n",
    "                item['link'] = str(response.url)\n",
    "\n",
    "                item['time'] = (str(datetime.now().year) + \"-\" + str(datetime.now().month) + \"-\" + str(\n",
    "                    datetime.now().day) + \" \" + str(datetime.now().hour) + \":\" + str(datetime.now().minute))\n",
    "\n",
    "                self.item_count += 1\n",
    "                if self.item_count > 15:\n",
    "                    raise CloseSpider('item exceeded')\n",
    "\n",
    "                yield item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST (BACK)\n",
    "    Completar por Javi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Codigo importante a explicar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
